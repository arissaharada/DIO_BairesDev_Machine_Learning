{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj7iFT+MWFIJa9wdUrE69W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arissaharada/DIO_BairesDev_Machine_Learning/blob/main/DIO_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Projeto de Transfer Learning em Python **\n",
        "O projeto consiste em aplicar o método de Transfer Learning em uma rede de Deep Learning na linguagem Python no ambiente COLAB.  \n",
        "\n",
        "Para exemplo, utilizaremos o seguinte projeto que realiza Transfer Learning com o Dataset do MNIST:\n",
        "https://colab.research.google.com/github/kylemath/ml4a-guides/blob/master/notebooks/transfer-learning.ipynb\n",
        "\n",
        "O dataset utilizado engloba duas classes: gatos e cachorros. Uma descrição da base de dados pode ser visualizada neste link: https://www.tensorflow.org/datasets/catalog/cats_vs_dogs.\n",
        "\n",
        "Já o dataset para download pode ser acessado por meio deste outro link:\n",
        "\n",
        "https://www.microsoft.com/en-us/download/details.aspx?id=54765."
      ],
      "metadata": {
        "id": "JMVfizji-6Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q kagglecatsanddogs_5340.zip -d /content/"
      ],
      "metadata": {
        "id": "dSiOcmRIkNtX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "cIb45KwJjmZo",
        "outputId": "8961d25b-f72a-4d46-8389-40e4f1f2e274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Iniciando limpeza e copy/resave das imagens para: /content/PetImages_clean\n",
            "  [Cat] processados: 2000/12501  (copiados até agora: 2000)\n",
            "  [Cat] processados: 4000/12501  (copiados até agora: 4000)\n",
            "  [Cat] processados: 6000/12501  (copiados até agora: 5999)\n",
            "  [Cat] processados: 8000/12501  (copiados até agora: 7999)\n",
            "  [Cat] processados: 10000/12501  (copiados até agora: 9999)\n",
            "  [Cat] processados: 12000/12501  (copiados até agora: 11998)\n",
            "  [Dog] processados: 2000/12501  (copiados até agora: 14499)\n",
            "  [Dog] processados: 4000/12501  (copiados até agora: 16499)\n",
            "  [Dog] processados: 6000/12501  (copiados até agora: 18498)\n",
            "  [Dog] processados: 8000/12501  (copiados até agora: 20498)\n",
            "  [Dog] processados: 10000/12501  (copiados até agora: 22498)\n",
            "  [Dog] processados: 12000/12501  (copiados até agora: 24497)\n",
            "Concluído. Imagens copiadas (limpas): 24998. Arquivos pulados por extensão: 2. Arquivos inválidos detectados: 2\n",
            "Exemplo de arquivos inválidos (até 20):\n",
            "  /content/PetImages/Cat/666.jpg\n",
            "  /content/PetImages/Dog/11702.jpg\n",
            "Total imagens válidas no CLEAN_ROOT: 24998\n",
            "Totals: 24998 train 17498 val 3750 test 3750\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,259,265\u001b[0m (8.62 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,259,265</span> (8.62 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916ms/step - accuracy: 0.8344 - auc: 0.9052 - loss: 0.3970"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 1s/step - accuracy: 0.8346 - auc: 0.9053 - loss: 0.3967 - val_accuracy: 0.9763 - val_auc: 0.9971 - val_loss: 0.1087 - learning_rate: 1.0000e-04\n",
            "Epoch 2/6\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910ms/step - accuracy: 0.9651 - auc: 0.9921 - loss: 0.1267"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 1s/step - accuracy: 0.9651 - auc: 0.9921 - loss: 0.1267 - val_accuracy: 0.9829 - val_auc: 0.9984 - val_loss: 0.0699 - learning_rate: 1.0000e-04\n",
            "Epoch 3/6\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890ms/step - accuracy: 0.9721 - auc: 0.9951 - loss: 0.0929"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 1s/step - accuracy: 0.9721 - auc: 0.9951 - loss: 0.0929 - val_accuracy: 0.9856 - val_auc: 0.9987 - val_loss: 0.0551 - learning_rate: 1.0000e-04\n",
            "Epoch 4/6\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886ms/step - accuracy: 0.9741 - auc: 0.9959 - loss: 0.0800"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 1s/step - accuracy: 0.9741 - auc: 0.9959 - loss: 0.0800 - val_accuracy: 0.9869 - val_auc: 0.9989 - val_loss: 0.0476 - learning_rate: 1.0000e-04\n",
            "Epoch 5/6\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923ms/step - accuracy: 0.9761 - auc: 0.9971 - loss: 0.0686"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m650s\u001b[0m 1s/step - accuracy: 0.9761 - auc: 0.9971 - loss: 0.0686 - val_accuracy: 0.9877 - val_auc: 0.9991 - val_loss: 0.0434 - learning_rate: 1.0000e-04\n",
            "Epoch 6/6\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902ms/step - accuracy: 0.9755 - auc: 0.9974 - loss: 0.0651"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 1s/step - accuracy: 0.9755 - auc: 0.9974 - loss: 0.0651 - val_accuracy: 0.9875 - val_auc: 0.9991 - val_loss: 0.0409 - learning_rate: 1.0000e-04\n",
            "Epoch 1/10\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9208 - auc: 0.9871 - loss: 0.1921"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m860s\u001b[0m 2s/step - accuracy: 0.9208 - auc: 0.9871 - loss: 0.1920 - val_accuracy: 0.9840 - val_auc: 0.9995 - val_loss: 0.0371 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9673 - auc: 0.9956 - loss: 0.0826"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m862s\u001b[0m 2s/step - accuracy: 0.9673 - auc: 0.9956 - loss: 0.0826 - val_accuracy: 0.9899 - val_auc: 0.9996 - val_loss: 0.0266 - learning_rate: 1.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m249/547\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6:34\u001b[0m 1s/step - accuracy: 0.9745 - auc: 0.9967 - loss: 0.0703"
          ]
        }
      ],
      "source": [
        "# Código completo: limpa imagens corrompidas salvando versões JPEG válidas e roda Transfer Learning\n",
        "# Rode no Colab (Runtime > Change runtime type > GPU)\n",
        "\n",
        "import os, random, shutil, math, time\n",
        "from pathlib import Path\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIGS\n",
        "# ----------------------------\n",
        "ORIG_ROOT = '/content/PetImages'       # pasta original que você descompactou\n",
        "CLEAN_ROOT = '/content/PetImages_clean'  # pasta onde vamos salvar imagens \"limpas\" (será criada)\n",
        "CLASSES = ['Cat', 'Dog']               # pastas esperadas em ORIG_ROOT\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 123\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "VALID_EXT = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tif', '.tiff')\n",
        "\n",
        "os.makedirs(CLEAN_ROOT, exist_ok=True)\n",
        "for c in CLASSES:\n",
        "    os.makedirs(os.path.join(CLEAN_ROOT, c), exist_ok=True)\n",
        "\n",
        "# ----------------------------\n",
        "# 1) varrer e \"corrigir\" imagens (re-salvar como JPEG convertidas para RGB)\n",
        "# ----------------------------\n",
        "print(\"1) Iniciando limpeza e copy/resave das imagens para:\", CLEAN_ROOT)\n",
        "start = time.time()\n",
        "bad_files = []\n",
        "copied = 0\n",
        "skipped = 0\n",
        "\n",
        "for cls in CLASSES:\n",
        "    orig_dir = os.path.join(ORIG_ROOT, cls)\n",
        "    clean_dir = os.path.join(CLEAN_ROOT, cls)\n",
        "    if not os.path.isdir(orig_dir):\n",
        "        raise FileNotFoundError(f\"Pasta esperada não encontrada: {orig_dir}\")\n",
        "    files = os.listdir(orig_dir)\n",
        "    for i, fname in enumerate(files):\n",
        "        if not fname.lower().endswith(VALID_EXT):\n",
        "            skipped += 1\n",
        "            continue\n",
        "        src = os.path.join(orig_dir, fname)\n",
        "        try:\n",
        "            with Image.open(src) as im:\n",
        "                im = im.convert('RGB')             # força 3 canais\n",
        "                # resave as JPEG into clean folder with unique name to avoid duplicates\n",
        "                base_name = Path(fname).stem\n",
        "                # evitar sobrescrever: acrescenta sufixo se já existir\n",
        "                out_name = f\"{base_name}.jpg\"\n",
        "                out_path = os.path.join(clean_dir, out_name)\n",
        "                k = 1\n",
        "                while os.path.exists(out_path):\n",
        "                    out_name = f\"{base_name}_{k}.jpg\"\n",
        "                    out_path = os.path.join(clean_dir, out_name)\n",
        "                    k += 1\n",
        "                im.save(out_path, format='JPEG', quality=95)\n",
        "                copied += 1\n",
        "        except (UnidentifiedImageError, OSError, ValueError, Exception) as e:\n",
        "            # imagem corrompida ou problema — registra e pula\n",
        "            bad_files.append(src)\n",
        "        # logging periódico para não inundar o output\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f\"  [{cls}] processados: {i+1}/{len(files)}  (copiados até agora: {copied})\")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f\"Concluído. Imagens copiadas (limpas): {copied}. Arquivos pulados por extensão: {skipped}. Arquivos inválidos detectados: {len(bad_files)}\")\n",
        "if bad_files:\n",
        "    print(\"Exemplo de arquivos inválidos (até 20):\")\n",
        "    for bf in bad_files[:20]:\n",
        "        print(\" \", bf)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) construir listas de arquivos a partir do CLEAN_ROOT (agora todas JPEG)\n",
        "# ----------------------------\n",
        "filepaths = []\n",
        "labels = []\n",
        "for idx, cls in enumerate(CLASSES):\n",
        "    p = os.path.join(CLEAN_ROOT, cls)\n",
        "    for fname in os.listdir(p):\n",
        "        if not fname.lower().endswith('.jpg'):\n",
        "            continue\n",
        "        filepaths.append(os.path.join(p, fname))\n",
        "        labels.append(idx)\n",
        "\n",
        "print(\"Total imagens válidas no CLEAN_ROOT:\", len(filepaths))\n",
        "\n",
        "# ----------------------------\n",
        "# 3) embaralhar e split (70/15/15)\n",
        "# ----------------------------\n",
        "random.seed(SEED)\n",
        "pairs = list(zip(filepaths, labels))\n",
        "random.shuffle(pairs)\n",
        "filepaths, labels = zip(*pairs)\n",
        "filepaths, labels = list(filepaths), list(labels)\n",
        "\n",
        "n = len(filepaths)\n",
        "train_end = int(0.7 * n)\n",
        "val_end = int(0.85 * n)\n",
        "\n",
        "train_files, train_labels = filepaths[:train_end], labels[:train_end]\n",
        "val_files, val_labels = filepaths[train_end:val_end], labels[train_end:val_end]\n",
        "test_files, test_labels = filepaths[val_end:], labels[val_end:]\n",
        "\n",
        "print(\"Totals:\", n, \"train\", len(train_files), \"val\", len(val_files), \"test\", len(test_files))\n",
        "\n",
        "# ----------------------------\n",
        "# 4) função de carregamento (usamos decode_jpeg já que re-salvamos como JPG)\n",
        "# ----------------------------\n",
        "def load_and_preprocess(path, label):\n",
        "    image_data = tf.io.read_file(path)\n",
        "    img = tf.io.decode_jpeg(image_data, channels=3)  # seguro: todas são JPG\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    img = preprocess_input(img)\n",
        "    return img, label\n",
        "\n",
        "# ----------------------------\n",
        "# 5) criação dos datasets (augmentation on-the-fly no train)\n",
        "# ----------------------------\n",
        "def make_dataset(files, labels, training=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
        "    ds = ds.map(lambda p, l: load_and_preprocess(p, l), num_parallel_calls=AUTOTUNE)\n",
        "    if training:\n",
        "        ds = ds.shuffle(2048, seed=SEED)\n",
        "        aug = keras.Sequential([\n",
        "            layers.RandomFlip(\"horizontal\"),\n",
        "            layers.RandomRotation(0.08),\n",
        "            layers.RandomZoom(0.08),\n",
        "        ])\n",
        "        ds = ds.map(lambda x, y: (aug(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_dataset(train_files, train_labels, training=True)\n",
        "val_ds = make_dataset(val_files, val_labels, training=False)\n",
        "test_ds = make_dataset(test_files, test_labels, training=False)\n",
        "\n",
        "# ----------------------------\n",
        "# 6) construir modelo MobileNetV2\n",
        "# ----------------------------\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(*IMAGE_SIZE, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape=(*IMAGE_SIZE, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "# ----------------------------\n",
        "# 7) callbacks e treino da \"cabeça\"\n",
        "# ----------------------------\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint('/content/best_model.h5', monitor='val_loss', save_best_only=True),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=6,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 8) fine-tuning\n",
        "# ----------------------------\n",
        "base_model.trainable = True\n",
        "fine_tune_at = len(base_model.layers) - 50\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[fine_tune_at:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
        ")\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,                  # treino adicional (ajuste conforme VRAM/time)\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 9) avaliar no test set e relatório\n",
        "# ----------------------------\n",
        "loss, acc, auc = model.evaluate(test_ds)\n",
        "print(\"Test loss:\", loss, \"acc:\", acc, \"auc:\", auc)\n",
        "\n",
        "# predições completas\n",
        "test_ds_full = tf.data.Dataset.from_tensor_slices((test_files, test_labels))\n",
        "test_ds_full = test_ds_full.map(lambda p, l: load_and_preprocess(p, l))\n",
        "test_ds_full = test_ds_full.batch(BATCH_SIZE)\n",
        "\n",
        "y_probs = model.predict(test_ds_full, verbose=1).ravel()\n",
        "y_pred = (y_probs >= 0.5).astype(int)\n",
        "y_true = np.array(test_labels)\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# salvar modelo final\n",
        "model.save('/content/final_model.h5')\n",
        "print(\"Modelos salvos: /content/best_model.h5 e /content/final_model.h5\")\n",
        "\n",
        "# mostrar algumas previsões\n",
        "import random\n",
        "n_show = 8\n",
        "idxs = random.sample(range(len(test_files)), n_show)\n",
        "plt.figure(figsize=(16,6))\n",
        "for i, idx in enumerate(idxs):\n",
        "    img = Image.open(test_files[idx]).convert('RGB').resize(IMAGE_SIZE)\n",
        "    plt.subplot(2,4,i+1)\n",
        "    plt.imshow(img)\n",
        "    prob = model.predict(np.expand_dims(preprocess_input(np.array(img).astype(np.float32)), axis=0))[0,0]\n",
        "    label_pred = CLASSES[int(prob >= 0.5)]\n",
        "    plt.title(f\"{label_pred} ({prob:.2f})\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ]
}